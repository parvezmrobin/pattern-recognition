{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict A Function's Output Using NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 1), 0.0061034694563633085, 9.98920784546378)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input = np.random.rand(150, 1) * 10\n",
    "\n",
    "X_input.shape, np.min(X_input), np.max(X_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input, layer_1_size, layer_2_size, num_output = 1, 10, 5, 1\n",
    "learning_rate = 0.01\n",
    "n_epoch = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"input\"):\n",
    "    X = tf.placeholder(tf.float32, [None, num_input], \"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\"):\n",
    "    W = tf.get_variable(\"W1\", [num_input, layer_1_size], tf.float32)\n",
    "    b = tf.get_variable(\"b1\", [layer_1_size], tf.float32)\n",
    "    Z = tf.add(tf.matmul(X, W), b)\n",
    "    A1 = tf.nn.relu(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer2\"):\n",
    "    W = tf.get_variable(\"W2\", [layer_1_size, layer_2_size], tf.float32)\n",
    "    b = tf.get_variable(\"b2\", [layer_2_size], tf.float32)\n",
    "    Z = tf.add(tf.matmul(A1, W), b)\n",
    "    A2 = tf.nn.relu(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"output\"):\n",
    "    W = tf.get_variable(\"W3\", [layer_2_size, num_output], tf.float32)\n",
    "    b = tf.get_variable(\"b3\", [num_output], tf.float32)\n",
    "    Z = tf.add(tf.matmul(A2, W), b)\n",
    "    Y_hat = Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\"):\n",
    "    Y = tf.placeholder(tf.float32, [None, num_output])\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.keras.losses.logcosh(y_true=Y, y_pred=Y_hat)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"optimize\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log(x)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sym = sympy.symbols(\"x\")\n",
    "func_str = \"log(x)\"\n",
    "func = sympy.sympify(func_str)\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 1), -5.09889790614919, 2.30150529476810)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_output(features) -> np.ndarray:\n",
    "    features = np.squeeze(features).tolist()\n",
    "    labels = []\n",
    "    for x in features:\n",
    "        labels.append(func.evalf(subs={x_sym: sympy.sympify(x)}))\n",
    "    return np.array(labels).reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 1), -5.09889790614919, 2.30150529476810)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_input = make_output(X_input)\n",
    "\n",
    "Y_input.shape, np.min(Y_input), np.max(Y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_input, Y_input, test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {X: X_train, Y: Y_train}\n",
    "test_dict = {X: X_test, Y: Y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "for _ in range(n_epoch):\n",
    "    session.run(optimizer, feed_dict=train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument 0.1844633 has invalid type <class 'numpy.float32'>, must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    283\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 284\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    285\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3576\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3577\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a float32 into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ab167517993c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1095\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \"\"\"\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m/Hard Disk/Projects/Python/pattern-recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    286\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    287\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    289\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument 0.1844633 has invalid type <class 'numpy.float32'>, must be a string or Tensor. (Can not convert a float32 into a Tensor or Operation.)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "cost = session.run(cost, feed_dict=test_dict)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sorted_index = np.argsort(np.squeeze(X_train))\n",
    "test_sorted_index = np.argsort(np.squeeze(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.plot(\n",
    "    np.squeeze(X_train)[train_sorted_index], \n",
    "    np.squeeze(Y_train)[train_sorted_index],\n",
    "    'b-'\n",
    ")\n",
    "plot.plot(\n",
    "    np.squeeze(X_test)[test_sorted_index], \n",
    "    np.squeeze(Y_test)[test_sorted_index],\n",
    "    'r-'\n",
    ")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
